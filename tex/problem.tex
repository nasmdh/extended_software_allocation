\section{Software-to-Hardware Allocation Problem}\label{sec_problem}
The software applications are allocated to the execution platform by mapping the software-component replicas (or software components)  \ttsss{Q} to the computing units $\nodes$. The software-to-hardware allocation problem is to find the mapping $\x:\sss{Q}\rightarrow\nodes$ that satisfies the user-defined requirements of the software applications, but also minimize the total power consumption of the applications, where $\textbf{x}$ is a possible mapping matrix, and {\ttxkij } represents the mapping of the software component \ttsss{q}[k][i,j] to the computing unit $n_h$, where $h=\xkij$, of the application $A_k$.

\subsection{Total Power Consumption}
The total power consumption of the applications $\mathcal{P}_{total}(\x)$ is computed as the sum of the power consumption of the computing units to which the applications are allocated. The power consumption of a unit is computed according to the linear model proposed by Fan et al.~\cite{Fan2007PowerComputer} as shown by Equation~(\ref{eqn_powerconsumption}), which is directly proportional to its load (or utilization) and is inductively formulated from experimental results.
\begin{equation}
\label{eqn_powerconsumption}
\mathcal{P}(u)=P_{idle} + (P_{busy}-P_{idle})*u,
\end{equation}
where $u$ is the utilization of a computing unit, $P_{idle}$ and $P_{busy}$, respectively refer to the power consumption measured at minimum and maximum processor load. The parameters of the model can be obtained by running performance benchmark suits, e.g., MiBench \cite{Guthaus2001MiBench:Suite}, AutoBench \cite{EMBC2018AutoBenchProcessors}, etc. 

The utilization of a computing unit is computed as a sum of the utilization of the tasks mapped to it $\ssb{T}[n_h]$, which are identified by traversing the mapping elements-wise \ttxkij{} as shown in Equation~(\ref{eqn_tasks_per_node}), where $\ssb{T}$ refers to the tasks implementing the component type $\sss{c}$.
\begin{align}
\label{eqn_tasks_per_node}
\ssb{T}[n_h]=\{\sss{T} | \forall kij\ h=\xkij \}   \mbox{  forall } h= 1,...,n_N
\end{align}

Then, the utilization of the units, indicated by the vector $(u_1,...,u_{n_N})$, is 
\[
	(u_1,...,u_{n_N})(\x)=\sum_{\tau\in \ssb{T}[n_h]}{\mathcal{U}(\tau,n_h), \mbox{ for all } h=1,..,n_{N}},
\] where $\mathcal{U}(\tau,n)=WCET_{\tau,n}/P_\tau$ computes utilization of the task $\tau$ on the unit $n$. Thus, the total power consumption of the applications is formulated as
\begin{align}
\label{eqn_total_power}
\mathcal{P}_{total}(\textbf{x})  = \sum_{h=1}^{n_N}{\mathcal{P}(u_h(\x))} \mbox{ for all } h=1,..,n_{N},  
\end{align}

\subsection{Tasks  and Messages Timing Constraints}
The tasks timing constraints ensure that the tasks in the distributed system meet their respective deadlines, that is 
$\forall \tau\in \ssb{T}[n_h] \ ResponseTime_\tau(\x)\leq \DL_\tau$ for all $h=1,...,n_N$, following Equation~(\ref{eqn_tasks_per_node}). Similarly, the messages timing constraints ensure that the response time of message in the CAN bus meet their respective deadlines. 

The set of messages in the bus is determined by traversing the edges of the tasks graphs. If the edge relates tasks located on different units, a message is used to communicate across the bus, otherwise, no message is used.

However, due to replication, the edge may represent a set of subedges, where each subedge relates tasks replicas on both sides of the edge. Consider $\sss{\mathcal{R}}[k][\tau][b]=\{(\tau,n)\in\tau\times\sss{N}[k][\tau]\}$ is the set of the replicas of type $\tau\in\gr{V}{\at}$, now assume $(t1,t2)\in\gr{E}{\at}$ is an edge in the task graph and  $\sss{\mathcal{R}}[k][t2][b], \sss{\mathcal{R}}[k][t2][b]$ are the replicas of type $t1$ and $t2$, respectively. The set of subedges between $t1$ and $t2$ is  $\sss{\mathcal{R}}[k][t2][b]\times\sss{\mathcal{R}}[k][t2][b]$, and the set of messages in these subedges are where the latter relates tasks replicas located on different nodes. By extension, the set of message in the bus is determined as,
\begin{equation}
\label{eqn_tasks_nodes}
M=\{m_{i,j} | \forall (t1,t2)\in \EE(\atx)\forall (i,j)\in\sss{\RL}[k][t1][b]\times\sss{\RL}[k][t2][b]\ n_i\neq n_j\},
\end{equation}
where $(i,j)$ is a sub-link of $(t1,t2)$, $m_{ij}$ is the message that the replica $i$ uses to communicate with the replica $j$, $n_i,n_j\in \nodes$.  

We assume the messages inherit the timing and criticality specifications of the sending tasks, thus $P_{m_i }= P_{t1}$, $\mathrm{CL}_{m_i}=\mathrm{CL}_{t1}$.

\subsection{End-to-end Timing Constraints}
The end-to-end timing constraints over \ttx ensure that the delays of the chains meet their respective end-to-end requirements, that is $\forall \gamma\in \ssp{\Gamma}\ Delay_\gamma(\x)\leq \sss{\EE}[k][\gamma]$ for all $k=1,...,n_A$. Note that, end-to-end constraints implicitly assumes the tasks and messages constraints are satisfied.

The delay calculation of a chain $\Gamma$ is multiplicity $\Gamma^*$ due to replication. Consider the chain $\Gamma=(\tau_1,...,\tau_l)$. The set of chains with replication is a cartesian product of the tasks replicas (or the tasks nodes mapping to computing units according to $\x$ in the chain, that is $\Gamma^*(\x)= \sss{\mathcal{R}}[k][\tau_1][b]\times,...,\times\sss{\mathcal{R}}[k][\tau_l][b]$, where $l$ is the chain length. Assume that we want to calculate the age delay of the chain $\gamma\in \Gamma^*$ compositionally, where $\gamma=(t_i)_{i=1}^l=(t_1,...,t_l)$: first we identify the subchains $I$ and messages $J$ in the chain. The subchains $I$ are subsets of the chain $\gamma$ where the communication between the sender and receiver tasks of the chain use a network bus. That is, if $t_i$ is the sender task, and its receiver task $t_{i+1}$ is mapped to a different unit, i.e., $n_{t_i}\neq n_{t_{i+1}}$, then $(t_h)_{h=i'}^i\in I$ is a subchain of $\gamma$ and $m_{t_i}\in J$ is the message used by the subchain, where $0\leq i'\leq i$, captured by the expression
$
	(I;J)=\{(t_i)_{i=0}^{l-1};m_{t_i} | n_{t_i}\neq n_{t_{i+1}}\}
$.
	
Thus, the delay $\Delta_\gamma(\x)$ for a mapping \ttx is computed as the sum of the age delays of its subchains and the response-times of the messages,
\[
	\Delta_{\gamma}(\x)=\sum_{i\in I_{\gamma}(\x)}{\Delta^{sub}_{i}(\x)} + \sum_{j\in J_{\gamma}(\x)}{\Big[\delta^{msg}_j(\x)+P_{suc(j)}\Big]},
\]
according to the age-delay formula shown in Equation (\ref{eqn_agedelay_multinode}), where $\Delta^{sub}$, $\delta^{msg}$ are the functions that compute the age delay of $i$ subchain, and the response-time of $j$ message, respectively. 

Thus, the chains timing constraints are formulated for a mapping \ttx is:
\begin{align}
\label{eqn_chains_constraints}
\forall \gamma \in \ssp{\Gamma^*(\x)} \ \sss{\Delta}[k][{\gamma}](\x) & \leq \sss{\EE}[k][\gamma],
\end{align}
\begin{example}[Delay Calculation] Consider the chain $\Gamma=\tau_1\rightarrow\tau_2\rightarrow\tau_4$ from Figure~\ref{fig_dag_tasks} where $\tau_1$ and $\tau_2$ realize the component types $c_1$, and $\tau_4$ realizes $c_2$. The mapping of the components is shown in Figure~\ref{fig_deployment} (b), i.e., with replication. Thus, the units to which $\tau_1$ and $\tau_2$ are mapped are $\sss{\mathcal{R}}[k][\tau_l][b]=\sss{\mathcal{R}}[k][\tau_2][b]=\{n_1,n_2\}$, and $\tau_4$ to $\sss{\mathcal{R}}[k][\tau_4][b]=\{n_2,n_3\}$, by infering the mappings of respective components. Table~\ref{tbl_chains_with_replication} illustrates how to compute the chains, considering replication of degree 2, which is $\Gamma^* = \sss{\mathcal{R}}[k][\tau_1][b] \times \sss{\mathcal{R}}[k][\tau_2][b] \times \sss{\mathcal{R}}[k][\tau_4][b]$, and also how to compute the subchains and messages of each chain $\gamma\in \Gamma^*$. The delays of the subchains is computed according to the age-delay semantics demonstrated in Subsection~\ref{subsec_cause-effect_chains}.
\end{example} 
\begin{table}[]
	\begin{tabular}{@{}lll@{}}
		\toprule
		$\gamma\in \Gamma^*$ & $i\in I_\gamma$ & $j\in J_\gamma$ \\ \midrule
		$(\tau_1,n_1)\rightarrow (\tau_2,n_1)\rightarrow(\tau_4,n_2)$ & $(\tau_1,n_1)\rightarrow (\tau_2,n_1),(\tau_4,n_2)$ & $m_{(\tau_2,n_1), (\tau_4,n_2)}$ \\
		$(\tau_1,n_1)\rightarrow (\tau_2,n_1)\rightarrow(\tau_4,n_3)$ & $(\tau_1,n_1)\rightarrow (\tau_2,n_1),(\tau_4,n_3)$ & $m_{(\tau_2,n_1), (\tau_4,n_3)}$ \\
		$(\tau_1,n_2)\rightarrow(\tau_2,n_2)\rightarrow(\tau_4,n_2)$ & $(\tau_1,n_2)\rightarrow(\tau_2,n_2)\rightarrow(\tau_4,n_2)$ & $\emptyset$ \\
		$(\tau_1,n_2)\rightarrow(\tau_2,n_2)\rightarrow(\tau_4,n_3)$ & $(\tau_1,n_2)\rightarrow(\tau_2,n_2),(\tau_4,n_3)$ & $m_{(\tau_2,n_2), (\tau_4,n_3)}$ \\ \bottomrule
	\end{tabular}
\caption{Chains with replication of degree 2 for the chain $\Gamma=\tau_1\rightarrow\tau_2\rightarrow\tau_4$, its subchains $I$ and messages $J$.}
\label{tbl_chains_with_replication}
\end{table}

\subsection{Approximation of Age Delay Calculation}\label{subsec_approximation_alg}
Due to the replication, the number of chains with replication per chain $\Gamma$ grows exponentially as the degree of the replication $D$ linearly increases, $|\Gamma|^D$. Likewise, the length of the chain has a polynomial effect on the number of replicated chains. Moreover, the age delay calculation is an exhaustive search as demonstrated in Subsection \ref{subsec_cause-effect_chains}. For these reasons, the age delay computation is sometimes prohibitively expensive considering the meta-heuristic algorithms, which compute large-space candidate solutions over thousands of iterations. Therefore, we propose an approximation algorithm to efficiently compute the delays based on the worst-case age delay explained in the following lemma:
\begin{lemma}[Worst-case Adge Delay] 
	The worst-case age delay of a chain mapped to a single computing unit follows the maximization of the age delay formula shown in Equation (). Thus, the source task reads from the buffer as early as possible and the sink task reads from predecessor task as late as possible. Moreover, the read operation in between the chain, we assume ``just miss'' and late execution before the deadline of the executing task, which means the worst-case age delay becomes the summation for the .... which occurs between the earliest activation of the source task and the latest activation of the sink task plus the maximum worst-case response time of the sink task of a chain. The earliest activation occurs when the source task start time concedes its release time, and latest activation time of the sink task occurs not latter than the second hyper-period since the data is picked up by the source task. 
\end{lemma}

The lemma provides the worst-age delay of a chain mapped to a single unit, however, the chains with the replication can mapped to over a network. So, the worst age delay considers the messages time and the time considered for the just miss from the receiving end of the task across the network. Therefore, the worst-case edge delay of the chain $\Gamma$ is calculated as follows: first we identify the chain-with-replication $\gamma\in\Gamma^*$ that uses the network frequently, then the worst-case edge delay is the summation of the summation of the worst-case edge delay mapped to a single unit $2H$, the response times of the messages $\sum{\delta^{msg}(\x)}$, and the ``just miss'' times $\sum{P_{suc(j)}(\x)}$.
\begin{align}
\Delta^{worst}=2H+\sum_{j\in J_{\gamma}(\x)}{\Big[\delta^{msg}_j(\x)+P_{suc(j)}(\x)\Big]},
\end{align}
where $\mbox{ where } H=lcm(\gamma)$ is the worst-case age delay over a single unit, $\Delta^{worst}$ is the worst-case age delay over multiple units, and $\exists \gamma\in\Gamma^*\ \max{|J_\gamma|}$ is the chain-with-replication that uses the network frequently.

\subsection{Software-Applications Reliability Constraints}\label{subsec_reliability_constraint}
The applications reliability constraints ensure the mapping $\textbf{x}$ satisfies the user-defined reliability requirements, that is $\rel_{A_k}(\x)\leq \RL_{A_k}$, for all $k=1...n_A$. 
The reliability of each application is computed over $t$ period of time from the computing units \ttssp{N} and  the shared network bus $B$, where \ttssp{N} hosts \ttar. The reliability is computed assuming exponentially distributed and constant failure rates of the units $\lambda_{n_h}$ as well as the network bus $\lambda_B$. Thus, the reliability of an application is computed as a product of the reliability of the units and the network bus as shown in Equation (\ref{eqn_appreliability_app}). Note that, if application does not use the shared bus, then $\rel_{B}=1$. Equation (\ref{eqn_nodes_app}) finds the units \ttssp{N} that the application \ttar uses by traversing the partition \ttx in linear time.
\begin{align}
\label{eqn_appreliability_app}
&Reliability_{\ar}(\x)=Reliability_{\ssp{\nodes}}(\x)*Reliability_\bus\\
\label{eqn_nodes_app}
&\ssp{\nodes}=\{e\in \nodes| \forall ij\ e=m_h\},\mbox{ where } h=\ssx{k}{ij}
\end{align}
We assume applications are mutually exclusive, i.e., no shared components exist between any two applications. Therefore, we can safety calculate the reliability of applications independently. Consequently, to increase readability, we remove the superscript $(A_k)$ in the rest of this subsection.

The reliability of the units is $Reliability_N(\x)=e^{-\lambda_N(\x) t}$, where $\lambda_N(\x)$ is the failure rate of an $N$-unit system over the partition \ttx. The system failure-rate is computed using the state enumeration as shown in \cite{Lucet1999ExactReliability}, which is an exact technique to calculate reliability, as opposed to using series-parallel technique motivated in Subsection \ref{sub_reliability}. By applying the state enumeration technique, the system failure-rate can be defined as the probability a software application \textit{fails} in the probability space $\langle \Omega,\xi,p,f\rangle$.
\begin{itemize}
	\item $\Omega=\{0,1\}$ are the possible outcomes (or states) of a computing unit. Assume the Boolean variable $s_h\rightarrow\Omega$, which indicates the state of $n_h$, then $s_h=0$ indicates $n_h$ fails and $s_h=0$ indicates $n_h$ operates. Thus, for computing units $N=\{n_1,..,n_{n_N}\}$, the states of the units (or configuration) is indicated by the $N$-cardinality set $S=\{s_1,...,s_{n_\nodes}\}$.
	
	\item $\xi=\Omega^S$ are elementary events that correspond to the possible configurations of the units $N$, therefore, the events are mutually exclusive. Consider $N=\{n_1,n_2,n_3\}$, Table \ref{tbl_application_rel} shows the possible configurations $\xi$. Assume the configuration $s\in \xi=\{0,1,0\}$, it shows $n_1$ and $n_3$ fail as indicated by $s_1=0,s_3=0$, respectively, and $n_2$ operates as indicated by $s_2=1$. 
	
	\item $p:\xi\rightarrow[0,1]$ assigns the configurations probabilities using
	\[\forall s\in \xi\  p_s=\prod_{h=1}^{n_N}{\lambda_{n_h}*(1-s_h)+(1-\lambda_{n_h})*s_h}\]
	where $\lambda_{n_h}$ is the failure-rate of $n_h$. The probability $p_s$ is the product of the probability of having the state $s_h$, which is $\lambda_{n_h}$ if $n_h$ fails, otherwise, $(1-\lambda_{n_h})$ if $n_h$ operates.
	
	\item $f:\xi\rightarrow \{0,1\}$ determines the status of the application in each state $s\in\xi$, that is $f_s=0$ means the application fails, otherwise, $f_s=1$ means the application operates at the sate $s$.
\end{itemize}

\begin{table}
	\centering
	\begin{tabular}{@{}llll@{}}
		\toprule
		Units Config. & Probability & Comonent Status & Application Status \\ 
		$s\in\xi$   & $p_s$     & $\forall i\ s_{c_i}$ & $f_s$ \\ \midrule
		$\{0,0,0\}$ & 0.0000000000 & $\{0, 0, 0\}$        & 0     \\
		$\{0,0,1\}$ & 0.0000000099 & $\{0, 0, 1\}$         & 0     \\
		$\{0,1,0\}$ & 0.0000000099 & $\{1, 0, 0\}$          & 0     \\
		$\{0,1,1\}$  & 0.0000999800 & $\{1, 1, 1\}$         & 1     \\
		$\{1,0,0\}$ & 0.0000000099 &$\{1, 0, 1\}$         & 0    \\
		$\{1,0,1\}$ & 0.0000999800 & $\{1, 1, 1\}$        & 1     \\
		$\{1,1,0\}$ & 0.0000999800 & $\{1, 1, 1\}$          & 1     \\
		$\{1,1,1\}$ & 0.9997000299 & $\{1, 1, 1\}$           & 1     \\ \bottomrule
	\end{tabular}
	\caption{Example of the application reliability calculation using state enumeration over 10-year operational lifetime: an application with component types $C=\{c_1,c_2,c_3\}$, replicas $Q=\{c_{1,1},c_{1,2};c_{2,1},c_{2,2};c_{3,1},c_{3,2}\}$ partitioned on $N=\{n_1,n_2,n_3\}$ according to Figure \ref{fig_deployment}, the variable $s_{c_i}\in\{0,1\}$ indicates if the replicas of type $c_i$ fails or functions, respectively.}
	\label{tbl_application_rel}
\end{table}

%The fact that an application functions $f_s$ is defined via its inverse, which is \textit{software application failure}, deductively as follows:
\begin{definition}[Software Application Failure]
	A software application fails in the configuration $s\in\xi$ if there exists a component type $c_i$ where all of its replicas $Q_i$ \textit{fail}, otherwise, it functions, as shown in Equation (\ref{eqn_app_failure}).  The component replica $q_i,j\in Q_i$ of type $c_i$ fails if $n_h$ fails, that is $s_h=0$.
	\begin{align}
	\label{eqn_app_failure}
	f_s(\x)&= 
	\begin{cases}                                           
	0 & \mbox{ if } \exists i\ c_i|\forall j\ s_h=0\\
	1 & \mbox{ otherwise }
	\end{cases}&\mbox{ where }h=x_{ij}
	\end{align}
\end{definition}

Thus, the failure rate of the $N$-unit system $\lambda_N(\x)$ is the sum of the probabilities in which the application fails, that is
\[
\lambda_N(\x)=\sum_{s\in \xi|f_s(\x)=0}p_s(\x)
\]
\begin{example}[Reliability Calculation]
	Let us assume we want to calculate the reliability of the application in Table~\ref{tbl_application_rel} over a 10-year (or 87600h) operational lifetime. The reliability of the units is $\rel_N=e^{-\lambda_N t}=0.99736671$, where $\lambda_N=p_1+p_2+p_3+p_5=0.0000000301$. Assume $\lambda_B=0.00000001$, hence $\rel_B=e^{-\lambda_B t}=0.99912438$. Then, the reliability of the application is $\rel_N*\rel_B=0.99649339932$.
\end{example}
\subsection{Software Allocation Optimization}\label{sec_allocation}
The software allocation is defined as a single-objective optimization problem. The objective function  $\mathcal{P}(\x)$ is a cost function which minimizes the total power consumption of the software applications as deployed in the heterogeneous computing units, where \ttx is the decision variable (or solution) of the optimization. The cost function is formulated in Equation \ref{eqn_optimization}, with inequality constraints shown by Equation (\ref{eqn_reliability}, \ref{eqn_responsetime},\ref{eqn_e2e}). The constraints ensure the solution meet the reliability requirements, the tasks deadlines,  and the end-to-end requirements of the chains. 
\begin{align}
\label{eqn_optimization}
\min_{\x\in X}\ \mathcal{P}_{total}(\x) & &\text{ subjected to:} \\
\label{eqn_reliability}
\rel_{A_k}(\x)&\leq \RL_{A_k} & \mbox{ forall } k=1,...,n_{A}\\
\label{eqn_responsetime}
\forall i\in T_{m_h}\    \sss{ResponseTime}[i][{\tau_i}](\x)&\leq \sss{\DL}[i][\tau_i]& \mbox{forall } h=1,...,n_{N}\\ 
\label{eqn_e2e}
\forall \gamma \in \ssp{{\Gamma^*}}\  \sss{Delay}[k][\gamma](\x)&\leq \sss{\EE}[k][\gamma]& \mbox{forall } k=1,...,n_{A}
\end{align}
where $X$ is the search space of the problem, $\textbf{x}\in X$ is a feasible solution, and $\xkij\in \textbf{x}$ is a mapping of a component $\sss{q}[k][i,j]$ to the node $m_h$, where $h=\xkij$
In the next section, we discuss our proposed method to address the considered optimization problem. 
