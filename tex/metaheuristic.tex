\section{Solution Using Hybrid Particle Swarm Optimization}
%Although the proposed ILP approach provides exact solutions, that is with an AUTOSAR software allocation with minimum power consumption, the approach approach does not scale well for large applications.
Although well-known ILP solvers such as CPLEX ILP Solver[cite to CPLEX] can be used to find exact solutions for the proposed ILP model, they all suffer from scalability problem. When it comes to allocation of large automotive applications, more scalable approaches such as heuristic and meta-heuristic algorithms are needed to find (near-)optimal solutions. 

In this section, we propose an meta-heuristic approach based on several metaheuristic techniques to address the scalability challenge. Metaheuristic techniques assumes little of the problem in question, and therefore are ideal to solve high dimensional and complex optimization problems, that is, problems that are difficult or practically impossible to solve by exact methods, e.g., linear programming, or exhaustive search methods, e.g., Branch and Bound (BB). They have improved over the last decades with respect to effectiveness, efficiency, and ease of use by providing fewer user-configurable parameters.

Metaheuristic techniques do not guarantee optimal solutions, nevertheless, the returned solutions can be good enough (or acceptable) in the eyes of the system designer, for this particular problem, it means although the power consumption of the system may not be optimal, the solution can be deemed acceptable, that is as long as not only the constraints are fulfilled but also the energy consumption is better than other naive heuristic algorithms. 
%In the opposite case, where the constraints could not be fulfilled, the algorithms can be rerun several times until the desired results are found, or the design can be relaxed by weakening the timing and reliability constraints of the system.

Metaheuristic techniques perform differently for different problem types, size, and complexity. In this section, we show application of different metaheuristic techniques employing swarm intelligence and evolutionary approaches in finding the (near) optimal solution for the considered allocation problem.
Specifically, we apply (i) Particle Swarm Intelligent (PSO), (ii) Differential Evolution (DE) techniques, (iii) hybrid PSO with DE, and (iv) hybrid PSO with Hill-climbing. The hybrid methods aim at improving the performance of the algorithms by exploiting the advantages of the combined algorithms. 

PSO has been applied to solve a wide range of problems, including a task allocation problem \cite{yin2007task}, and DE is shown to scale well for problems with high dimensions. In fact, PSO and DE are used together for improved performance in several optimization problems [some references are needed], likewise, PSO is used with local search techniques such as Hill climbing to intensify the search [some ref are needed]. Finally, we evaluate the different metaheuristic methods based on solution quality and computation time for different software allocation problems.

A meta-heuristic algorithm composes of two major parts, solution representation and a fitness function. The solution representation shows the data structure that is used to represent each point in the problem space and having a significant impact on the performance of the meta-heuristic algorithm. The fitness function is used to evaluate the quality of candidate solutions based on their fitness to meet the problem's objective. A solution that delivers lower power consumption and violates less constraints is indicated by a lower fitness value. In the following, we describe the solution representation and the fitness function proposed in our solution framework to run the meta-heuristic algorithms. 

\subsection{Solution Representation}
In contrast to the \textit{0-1} representation, the integer-linear representation uses much lower number of variables, that is $N*K(L-1)$, e.g., for a software allocation problem with $N=10,L=8,K=2$, 140 variables are saved. Of course the possible values in the former representation is two whereas in the latter representation, it is $L$, which is usually higher and results larger solution space. Nevertheless, the integer-linear representation is compact and computationally more efficient.

\subsection{Fitness Function Definition}
% Define the fitness function
 The fitness function $f(\textbf{x})$ should include both the objective function $P(\textbf{x})$ and the constraints such as $Timing(\textbf{x}),Reliability(\textbf{x})$ in order to compute real-valued numbers that indicate the quality of the candidate solutions. 
 
In the context of meta-heuristics, it is highly desirable to integrate the goal function and all constraints into one function that can be
used as a fitness function. [cite to talbi2009metaheuristics and faragardi2018efficient].

%@book{talbi2009metaheuristics,
%  title={Metaheuristics: from design to implementation},
%  author={Talbi, El-Ghazali},
%  volume={74},
%  year={2009},
%  publisher={John Wiley \& Sons}
%}

%article{faragardi2018efficient,
 % title={An efficient placement of sinks and SDN controller nodes for optimizing the design cost of industrial IoT systems},
 % author={Faragardi, Hamid Reza and Vahabi, Maryam and Fotouhi, Hossein and Nolte, Thomas and Fahringer, Thomas},
 % journal={Software: Practice and Experience},
 % volume={48},
 % number={10},
 % pages={1893--1919},
 % year={2018},
 % publisher={Wiley Online Library}
%}

The benefit of using a single function, including all penalty functions, is to provide a metric to distinguish between two unfeasible solutions. For example, let us assume that $A$ and $B$ are two different solutions for the allocation problem while both violate
some constraints of the problem. Let us also assume that solution
$A$ slightly violates only one constraint, whereas solution
$B$ significantly violates multiple constraints. If the heuristic algorithm can perceive the difference between $A$ and $B$
in terms of being far away from a feasible solution, it guide the search toward a feasible solution more efficiently, compared to the case that the heuristic algorithm only knows that they are both infeasible. The integration of the goal function with all
the penalty functions is a promising solution to provide knowledge about how far an unfeasible solution is from a feasible
solution. 

Consequently, the original constrained optimization problem is transformed into unconstrained optimization problem, by extending the objective function $P(\textbf{x})$ with the constrains, represented by a set of \textit{penalty functions} $\{\phi_{reliability}(\textbf{x}),$  $\phi_{deadline}(\textbf{x}),$ $\phi_{e2e}(\textbf{x}), \phi_{rep}(\textbf{x})\}$. The first penalty function corresponds to the reliability constraint which returns 0 if the reliability constrain is not violated, otherwise returns a positive number denoting how far the reliability constraint is violated. The further violation, the higher value of the penalty function.
Similarly, the other penalty functions correspond to the deadline, the end-to-end timing requirement, and the replication constraints, respectively. 
Indeed, the penalty functions penalize the candidate solution by increasing its fitness (for our minimization problem), thus discriminating the solution. Section~\ref{sec:penaltyfunction} explains how our solution framework formulates the penalty functions.

The fitness function $f(x)$ is computed as follows.
%The fitness function is a combination of $\beta\sum{g_i(\textbf{x})}$ and $\gamma h(\textbf{x})$ functions, respectively computes the timing violations and a software application reliability violation, and each function is weighted to indicate the size of the penalty separately. Moreover, the penalty function $\Phi({\textbf{x}})$ is weighted to indicate the size of penalty that imposed on the combined violations of timing and reliability.

%\begin{align}
%\label{}
%    \min_{\textbf{x}\in X}\;\;& f(\textbf{x})=P(\textbf{x}) + \alpha %\Phi(\textbf{x})\\
%    \label{eqn_penalityfunc}\Phi(\textbf{x}) &= \sum\phi_i(\textbf{x})
%\end{align}

\begin{equation}
\label{}
    \min_{\textbf{x}\in X}\;\;& f(\textbf{x})=P(\textbf{x}) + \beta_1 \phi_{reliability}(\textbf{x}) + \beta_2 \phi_{deadline}(\textbf{x}) + \beta_3 \phi_{e2e}(\textbf{x}) + \beta_4 \phi_{rep}(\textbf{x}) 
\end{equation}
%The fitness function is a combination of $\beta\sum{g_i(\textbf{x})}$ and $\gamma h(\textbf{x})$ functions, respectively computes the timing violations and a software application reliability violation, and each function is weighted to indicate the size of the penalty separately. Moreover, the penalty function $\Phi({\textbf{x}})$ is weighted to indicate the size of penalty that imposed on the combined violations of timing and reliability.
where $\beta_1$ to $\beta_4$ are penalty coefficients used to tune the weight of the penalty functions with regard to the range of the objective function. In Section~\ref{sec:penaltycoefficient}, the proper value of the penalty coefficients is discussed in more details.

\subsection{Penalty Function}
\label{sec:penaltyfunction}

\begin{equation}
\label{}
    \phi_{reliability}(\textbf{x}) = \sum_{k=1}^{n_{A_k}}{Max \{0, \rel_{A_k}(\x) - RelReq_{A_k} \}}
\end{equation}

\begin{equation}
\label{}
    \phi_{deadline}(\textbf{x}) = \sum_{\forall \tau\in T_{m_h}}{Max \{0, ResponseTime_{\tau}(\x) - Deadline_{\tau} \}}
\end{equation}

\begin{equation}
\label{}
    \phi_{e2e}(\textbf{x}) = \sum_{\forall \gamma \in \ssp{\Gamma}}{Max \{0, Delay_\gamma(\x) - E2eReq_\gamma \}}
\end{equation}

\begin{equation}
\label{}
    \phi_{rep}(\textbf{x}) = \sum_{\forall c_i}{\sum_{\forall n_j}{\sum_{k=1}^{n_{rep}-1}{\sum_{k'=k+1}^{n_{rep}} x_{ij}^{(k)} x_{ij}^{(k')}}}}
\end{equation}


%\rel_{A_k}(\x)&\leq RelReq_{A_k} & \mbox{ forall } k=1,...,n_{A_k}\\
%\label{eqn_responsetime}
%\forall \tau\in T_{m_h}\    ResponseTime_{\tau}(\x)&\leq %Deadline_{\tau}& \mbox{forall } h=1,...,n_{M}\\ 
%\label{eqn_e2e}
%\forall \gamma \in \ssp{\Gamma}\  Delay_\gamma(\x)&\leq E2eReq_\gamma& %\mbox{forall } k=1,...,n_{A}\\
%\forall k\forall ij\ x_{ij}^{(k)}&\neq x_{ij}^{(k')},&  \mbox{ where } %k\neq k'=1,...,n_{rep}

\subsection{Penalty Coefficients}
\label{sec:penaltycoefficient}

\subsection{Metaheuristic Algorithms}
\subsubsection{Particle Swarm Optimization}
% What is PSO
PSO is a population-based technique proposed by Eberhart and Kennedy in 1995 to study social behavior, as inspired by natural swarm intelligence observed from the flocking of birds and schooling of fishes \cite{Kennedy1995ParticleOptimization}. Since then, it is extended in order to address various metaheuristic optimization challenges, such as intensification, diversification, convergence analysis, local optima, parameter tuning, computation time, etc. It is successfully applied on several complex real-world problems, e.g., diagnosis and classification of diseases, efficient engineering designs, tuning control design parameters, scheduling problems, etc \cite{Poli2008AnApplications}. 

In PSO, the population (or swarm) $PN=\{p_1,p_2,…,p_N\}$ is a collection of particles $p_i\in PN$, organized according to a certain population topology \cite{Liu2016TopologyOptimization}. A particle has a position $\textbf{x}$ and a velocity $\textbf{v}$, which denote current location and direction of the particle's motion, and current momentum, respectively. It is a memory-based technique, that is, it remembers the best performance of every particle as well as the best performance of the swarm $\textbf{z}$ in order to plan for the next move of the particles, where $\textbf{y},\textbf{z}$ are position vectors and have the same dimensions as $\textbf{x}$. The velocity of a particle is the resultant vector of its current velocity and the particles attraction vectors $(\textbf{y}-\textbf{x}), (\textbf{z}-\textbf{x})$, respectively, known as \textit{cognitive} and \textit{social} components of the  particle's velocity formula, as shown in Equation \ref{eqn_pso_velocity}. The attraction vectors impose force of attraction on the particle to move closer to their respective components. Thus, the next position of a particle is the resultant of its current position and its next velocity as shown in Equation (\ref{eqn_pso_position}).
\begin{align}
    \label{eqn_pso_velocity}
    \textbf{v} &\leftarrow  \omega\textbf{v} + c_1Rand()\circ(\textbf{z}-\textbf{x}) + c_2Rand()\circ(\textbf{z}-\textbf{x})\\
    \label{eqn_pso_position}
    \textbf{x} &\leftarrow \textbf{x} + \textbf{v}
\end{align}
where $\omega$ is the weight of the velocity, also known as \textit{inertia coefficient}, and controls the convergence of the algorithm, $c_1, c_2$ are acceleration coefficients and controls the weight of attraction towards the cognitive and social components, respectively, $Rand()\in U(0,1)$ is a random function, along the acceleration coefficients, is element-wise multiplied with the components to improve diversity of the search by introducing stochastic behavior.

Although PSO was originally proposed for continuous problem, it is applied to discrete problems successfully as well. In the latter case, the solutions are represented by \textit{0-1} integer variables \cite{KennedyAAlgorithm} or integer-linear by approximation to the nearest integer values \cite{Clerc2000DiscreteProblem}, which is the representation employed adopted in our problem as it is compact, hence fewer decision variables. Accordingly, after the new position (or candidate solution) is determined, following Equations \ref{eqn_pso_velocity} and \ref{eqn_pso_position}, the solution is discretized by rounding off the its elements to the nearest integer values, that is $\textbf{x}\leftarrow [\textbf{x}]$.



% Show the representation of the a solution
% Demonstrated it on the example
% \begin{algorithm}
% \caption{PSO Algorithm}\label{alg_pso}
% \begin{algorithmic}[1]
% \Require n
% \Ensure Near (Optimal) Solution
% \State $x_{sb}\leftarrow$ getWorstPosition()
% \State $Particles\leftarrow$ createParticles($n$)
% \State initParticles($Particles$)
% \While{$!stoppingCritera$}
%     % Calculate personal best, swarm best positions
%     \State \Comment{Calculate personal best and swarm best positions of the particles}
%     \ForAll{$p \in Particles$}
%         \State $particle \leftarrow$  getPosition($p$)
%         \State $x_{pb} \leftarrow$  getPosition($p$) 
%         \If{$fitness(x)\leq fitness(x_{pb})$} \Comment{Personal best position}
%             \State $x_{pb}\leftarrow x$
%             \If{$fitness(x)\leq fitness(x_{sb})$} \Comment{Swarm best position}
%                 \State $x_{sb}\leftarrow x$
%             \EndIf 
%         \EndIf 
%     \EndFor
%     % Calculate next positions of the particles
%     \State \Comment{Calculate next positions of the particles}
% 	\ForAll{$p \in Particles$}
%         \State $v\leftarrow v+c1*r1*(x_{pb}-x)+c2*r2*(x_{sb}-x)$ 
%         \State $x\leftarrow$ getPosition($p$)$ + v$
%         %\State setParticlePosition($p,x$)
%     \EndFor
% \EndWhile
% \end{algorithmic}
% \end{algorithm}\vspace{-0.2cm}

\subsubsection{Differential Evolution}
Similar to PSO, Differential Evolution (DE) is a population-based metaheuristic technique for the global optimization which includes non-linear and non-differentiable problems. It was initially proposed by Storn and Price in 19995 \cite{Storn1997DifferentialSpaces}, since then it has improved with regard to the different operators of DE such as mutation and crossover, and variants over population topology and hybridization \cite{Das2016RecentSurvey}. It is a parallel search technique, therefore, is ideal for computationally intensive problems, and employs mutation and crossover operators that allow the search to skip local minima as opposed to PSO.

In every generation, the population undergoes mutation, crossover, and selection according to the formulas shown in Equation , (\ref{eqn_de_crossover}), and (\ref{eqn_de_selection}), respectively. A mutant vector $v$ is created from randomly selected elements $\{a,b,c\}\in PN$ according the mutation operation shown in Equation (\ref{eqn_de_mutation}), that is by adding the base matrix to the weighted difference matrix $F\circ(b-c)$, where $F$ controls the amplification of the $(\textbf{b}-\textbf{c})$ variation.
\begin{align}
    \label{eqn_de_mutation}
    \textbf{v} & \leftarrow   \textbf{a} + F\circ(\textbf{b}-\textbf{c})\\
    \label{eqn_de_crossover}
    u_{ik} & \leftarrow 
    \begin{cases}
    v_{ik} & \mbox{if } U(0,1) \leq CF \mbox{ and } h = (i*K+k)\\
    x_{ik} & \mbox{if } U(0,1) > CF \mbox{ and } h \neq (i*K+j)
    \end{cases}\\
    \label{eqn_de_selection}
    \textbf{x} &\leftarrow 
    \begin{cases}
    \textbf{u} & \mbox{if } f(\textbf{u}) < f(\textbf{x})\mbox{ functions}\\
    \textbf{x} & \mbox{otherwise }
    \end{cases}
\end{align}

\subsubsection{Hybrid Particle Swarm Optimization}
The canonical PSO technique uses the constriction factors to balance exploitation and exploration of the search space, that is to deliver better quality solutions. Nevertheless, it still suffers from local minima especially for complex and large problems that exhibit especially multimodal behavior. Hybridization of PSO is one the most widely studied approach in the improvement of the the PSO technique. Basically, it combines other optimization techniques, for instance to intensify local search, and improve diversification by introducing stochastic search. However, hybridization of PSO usually incurs additional computation time. Therefore, the benefit of hybridization has to be studied carefully in conjunction to computation time. Moreover, it should not complicate the user-configurable parameters, to be inline with the philosophy of PSO for ease-of-use.

PSO is hybridized with several optimization techniques, such as Genetic Algorithm (GA), DE, local searches (e.g., Hill-climbing, gradient decent, etc.), ant colony, simulated annealing, etc. Of which, it is shown to perform better when hybridized with DE on constrained, discrete, large benchmarks. Furthermore, it is shown to perform better when hybridized with Hill-climbing (specifically \textit{Steepest-descent} variant)for software allocation problem \cite{} in particular. In this paper, we hybridize PSO with DE (DEPSO) and Hill-climbing (HCPSO) to the solve the software allocation problem as formulated in Equation (x). In the latter case, we also apply the stochastic variant of Hill-climbing (SHPSO) in order to offset stagnation of the steepest Hill-climbing when applied on large software allocation problems.

\IncMargin{1em}
\begin{algorithm}[H]
\SetKwData{P}{P}\SetKwData{S}{sBest}
\SetKwData{Generation}{Generation}
\SetKwData{Interval}{Interval}
\SetKwData{Particles}{Particles}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwFunction{OptimizeUsingDE}{optimizeUsingDE}
\SetKwFunction{OptimizeUsingHC}{optimizeUsingHC}
\SetKwFunction{OptimizeUsingSHC}{optimizeUsingSHC}
\SetKwFunction{ComputeParticleVelocity}{computeParticleVelocity}
\SetKwFunction{ComputeParticlePosition}{computeParticlePosition}
\SetKwFunction{InitPSO}{initPSO}

\BlankLine
\Input{PSO parameters, DE parameters}
\Output{Software allocation solution \S .\textbf{x}}
\BlankLine
\Particles $P$ $\leftarrow$ \InitPSO{}\;
\BlankLine
 \While{termination criteria}{
  $P$ $\leftarrow$\ComputePersonalBest{$P$}\;
  \S $\leftarrow$\ComputeSwarmBest{$P$}\;
  \BlankLine
   \ForEach{$p\in P$}{
        \ComputeParticleVelocity{$p$} according to Equation (\ref{eqn_pso_velocity})\;
        \ComputeParticlePosition{$p$} according to Equation (\ref{eqn_pso_position})\;
   }
   \If{interval criteria}{
        $P$ $\leftarrow$ \OptimizeUsingDE{$P$}\;
    \tcp{$P$ $\leftarrow$ \OptimizeUsingHC{$P$}}\label{hc}
    \tcp{$P$ $\leftarrow$ \OptimizeUsingSHC{$P$}}\label{sh}
   }
 }
 \caption{Hybrid PSO Algorithms.}\label{alg_depso}
\end{algorithm}\DecMargin{1em}
 
\subsubsection{Differential Evolution PSO (DEPSO)}
DE complements the classical PSO by introducing stochastic behavior via the evolutionary operators such as mutation, cross-over and selection. In this specific hybridization approach, we allow the DE algorithm to run intermittently for some number of generations before the next PSO generation starts.

\subsubsection{Hill-climbing PSO}
Hill-climbing is a popular local search based on the notion of \textit{neighborhood}, that is, the candidate solution (or neighbor) that performs better is selected iteratively until no improvements can be made. The software allocation solution $\textbf{x}$ is neighbor to $\textbf{x\textquotesingle}$ if $\textbf{x}=\textbf{x\textquotesingle}$ except $\exists i,j|\;x_{ij}\neq x\textquotesingle_{ij}$, that is, a single mapping is different. In every iteration, the best neighbor is selected, and subsequently replaces the current candidate solution if it performs better, and continues until maximum iteration, this variant is known as Steepest-descent Hill-climbing (SHC).

Since SHC exhaustively checks all neighbors before moving to the next iteration, the computation time is high especially for high-dimensional problems. To offset this problem, we also apply the stochastic version of Hill-climbing. In the later case, the neighbor is selected randomly, first by selecting the dimension, that is the component $c_{ij}$, where $i=U(1,I)$ and $j=U(1,K)$, second, selecting the value, that is the node $n_j$, where $j=U(1,J)$. If the neighbor improves the current candidate solution sufficiently, the search moves to the next iteration, which is until no more improvements can be made.

